{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook - install this package\n",
    "(i.e., in the terminal, inside this folder run:\n",
    "\"pip install -e .\")\n",
    "Also make sure that prfpy is installed\n",
    "& that you are running the correct kernel\n",
    "\n",
    "If you have your own data, I would reccomend loading in 100 voxels first. Visually inspect them to check for the kind of bumps that display visual responses. Then run the analysis to check that everything is working.\n",
    "For full brain data, you will probably want to run on a cluster if it is available. But it is also doable locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prfpy.stimulus import PRFStimulus2D\n",
    "from prfpy.model import Iso2DGaussianModel\n",
    "from prfpy.fit import Iso2DGaussianFitter\n",
    "from prfpy.rf import gauss2D_iso_cart\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from marcus_prf_eg.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a simple example of prfpy use, fitting a Gaussian model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up the model it is important to be clear what settings you are using, and make it easily reproducible. To that end it is recommended that you keep all the specific values in a .yml file, which can be loaded in (rather than hard coding \"magic\" numbers in your script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to load the settings\n",
    "prf_settings = './fit_settings_prf.yml'\n",
    "with open(prf_settings) as f:\n",
    "    prf_info = yaml.safe_load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating stimulus object\n",
    "Before fitting the PRFs we need to create the design matrix, i.e., a binary representation of the stimulus. The stimuli used in this example is a bar moving across the visual field (see eg_screen_shot.png for an example). \n",
    "\n",
    "prfpy needs the design matrix to be a numpy array of n x n x time points. Each time point will correspond to the timepoints in the fMRI sequence. n is the number of pixels in the design matrix. \n",
    "\n",
    "Going from the stimulus shown in the scanner to the n x n x timepoints in the design matrix involves 3 steps:\n",
    "\n",
    "[1] Binarize the stimulus (where there is stimulus=1, no stimulus=0)\n",
    "\n",
    "[2] Make it square (i.e., cut off the unstimulated part of the screen). This is because typically the stimuli are presented in a circular aperture; therefore including the full rectangle of the screen is unnecessary. \n",
    "\n",
    "[3] Downsample the stimulus (so that it can run faster, e.g., from 1080 x 1080 to 100 x 100)\n",
    "\n",
    "If you want you could also just recreate the design matrix from same parameters you used to make the stimulus (i.e., defining bar/wedge position over time). This doesn't matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can use a function from Jurjen to do this... This may take a couple of minutes\n",
    "# dm_from_screenshots = get_prfdesign(\n",
    "#     screenshot_path=prf_info['scr_shot_path'],            # Path to the screenshots, 1 per TR\n",
    "#     n_pix = prf_info['n_pix'],                            # How many pixels do we want in the design matrix? (fewer means faster, default is 100 x 100)\n",
    "#     dm_edges_clipping = prf_info['dm_edges_clipping'],    # Do we want to clip any of the edges of the design matrix? Default is no...\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have the design matrix as a numpy array, you can simply load it...\n",
    "dm = np.load('./design_matrix.npy')\n",
    "\n",
    "# You can see we have a binarized matrix, of a bar moving across the screen...\n",
    "fig = plt.figure()\n",
    "rows = 10\n",
    "cols = 10\n",
    "fig.set_size_inches(15,15)\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(rows, cols, i+1)\n",
    "    ax.imshow(dm[:,:,i], vmin=0, vmax=1)\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to enter the design matrix in a way that prfpy can read it. \n",
    "# We do this using the PRFStimulus2D object\n",
    "# Information we need\n",
    "# -> Screen size (height), and distance (to the eye)\n",
    "# This is so that we can convert into degrees of visual angle (dov)\n",
    "# -> TR: This is so that we know the time (in s), of each 'frame' / pt in the time series\n",
    "# Here we have 225 time points. So total length of scan was TR * 225 (1.5*225) \n",
    "prf_stim = PRFStimulus2D(\n",
    "    screen_size_cm=prf_info['screen_size_cm'],          # Distance of screen to eye\n",
    "    screen_distance_cm=prf_info['screen_distance_cm'],  # height of the screen (i.e., the diameter of the stimulated region)\n",
    "    design_matrix=dm,                                   # dm (npix x npix x time_points)\n",
    "    TR=prf_info['TR'],                                  # TR\n",
    "    )\n",
    "print(f'Screen size in degrees of visual angle = {prf_stim.screen_size_degrees}')\n",
    "\n",
    "# If for some reason you do not have the original dimensions, but you do have the dov, you can use this calculator:\n",
    "# https://elvers.us/perception/visualAngle/ to make up some values for screen_size and distance, which will work...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "The next step is to prepare the fMRI time series. The format that prfpy wants is a numpy array of number of \n",
    "For prfpy the time series needs to be 2D numpy array, where the first dimension is units (i.e., voxels or vertices) and the second dimension is time. The number of timepoints in your data should match the number in the design matrix (here it is 225). \n",
    "\n",
    "A couple of notes:\n",
    "\n",
    "[*] Preprocessing? I assume that all of the important stuff, i.e., denoising, removing confounds has been done already. This is just to prepare the data for prfpy specifically. \n",
    "\n",
    "[*] Averaging? The more runs (i.e., repetitions of the same stimulus) you average over, the less noise, and the better the prf estimates will be. You can fit on a single run (especially in high SNR situations, e.g., using 7T) without averaging, but quality will be less good. \n",
    "\n",
    "[*] Voxels or vertices? In principle you can fit any time series data with a prf model. I always fit using vertices, with the data sampled to the cortical surface. This will make visualisations much easier, if you want to plot your PRF parameters on the cortical surface. You can then use tools like pycortex\n",
    "\n",
    "[*] Percent signal change? Again, in principle you can fit any time series data with a prf model. I use percent signal change, with the baseline set to 0. Why do this? It makes it easier to compare timeseries across voxels when the units are the same (as opposed to the arbitrary values you get out of \"raw\" fMRI data). Also, if you do *not* set the baseline value to 0 (i.e. the amplitude of the prf model when there is 0 stimulation) you need to fit it for each vertex. There are also added complications when you have PRFs with inhibitory components. Another option is to use z-scoring rather than psc. \n",
    "\n",
    "[*] Include first part of timeseries? Some people will remove the first (e.g.,5) timepoints of the fMRI data. This is because perhaps it takes a couple of seconds for the subject to get used to the scanner / stimulus or maybe there are startup effects... If you want to do this, make sure you make the corresponding changes to the design matrix.  \n",
    "\n",
    "### Steps, using example data\n",
    "I have provided some example time series data for 100 vertices\n",
    "\n",
    "[1] Convert to percent signal change\n",
    "\n",
    "[2] Set the median value during the baseline condition (i.e., when there is no stimulation) to zero\n",
    "\n",
    "See functions in marcus_prf_eg/utils for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = np.load('ts_run_1.npy') # Load the example data\n",
    "run2 = np.load('ts_run_2.npy') # Diferent runs...\n",
    "\n",
    "# REDO 0-10\n",
    "# run1 = run1[0:10,:]\n",
    "# run2 = run2[0:10,:]\n",
    "\n",
    "TR_s = prf_info['TR']\n",
    "time_pts = np.arange(run1.shape[1]) * TR_s\n",
    "i_vx = 0\n",
    "plt.plot(time_pts, run1[i_vx,:])\n",
    "plt.plot(time_pts, run2[i_vx,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example, raw time series. You can already see the peaks corresponding to the bar passes. \n",
    "But the units are arbitrary... We want to change it to be in psc\n",
    "Also we can improve the SNR by averaging over the 2 runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the averaging and baselining...\n",
    "# During the first 20 time points, there is NO stimulation. So we use this to set the baseline\n",
    "psc_avg_ts = raw_ts_to_average_psc(\n",
    "    raw_ts=[run1, run2],        # Enter the runs as a list of np.ndarrays\n",
    "    baseline=19,                # The first 20 time points, there is NO stimulation. So we use this to set the baseline\n",
    ")\n",
    "\n",
    "print('''Now we can see what the the time series looks like after psc, baselining and averaging:''')\n",
    "\n",
    "plt.plot(time_pts, psc_avg_ts[i_vx,:])\n",
    "plt.plot(time_pts, time_pts*0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the (gaussian) model\n",
    "Now we can create the PRF model. The simplest is the 2D isometric (i.e., circular) gaussian\n",
    "\n",
    "\n",
    "The Iso2DGaussianModel class is used to create an 2D gaussian model instance.\n",
    "There are a few parameters you can set. See below (copied from prfpy documentation), for details.\n",
    "Note you can also fit the HRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    \n",
    "    \"\"\"__init__ for Iso2DGaussianModel\n",
    "    constructor, sets up stimulus and hrf for this Model\n",
    "    Parameters\n",
    "    ----------\n",
    "    stimulus : PRFStimulus2D\n",
    "        Stimulus object specifying the information about the stimulus,\n",
    "        and the space in which it lives.\n",
    "    hrf : string, list or numpy.ndarray, optional\n",
    "        HRF shape for this Model.\n",
    "        Can be 'direct', which implements nothing (for eCoG or later convolution),\n",
    "        a list or array of 3, which are multiplied with the three spm HRF basis functions,\n",
    "        and an array already sampled on the TR by the user.\n",
    "        (the default is None, which implements standard spm HRF)\n",
    "    filter_predictions : boolean, optional\n",
    "        whether to high-pass filter the predictions, default False\n",
    "    filter_type, filter_params : see timecourse.py\n",
    "    normalize_RFs : whether or not to normalize the RF volumes (generally not needed).\n",
    "'''\n",
    "gg = Iso2DGaussianModel(\n",
    "    stimulus=prf_stim,                                  # The stimulus we made earlier\n",
    "    hrf=prf_info['hrf']['pars'],                        # These are the parameters for the HRF that we normally use at Spinoza (with 7T data). (we can fit it, this will be done later...)\n",
    "    filter_predictions = prf_info['filter_predictions'],# Do you want to filter the predictions? (depends what you did to the data, try and match it... default is not to do anything)\n",
    "    normalize_RFs= prf_info['normalize_RFs'],           # Normalize the volume of the RF (so that RFs w/ different sizes have the same volume. Generally not needed, as this can be solved using the beta values i.e.,amplitude)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the gaussian fitter\n",
    "Now we need to make a fitter, to load in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = Iso2DGaussianFitter(\n",
    "    data=psc_avg_ts,            # time series\n",
    "    model=gg,                   # model (see above)\n",
    "    n_jobs=prf_info['n_jobs'],  # number of jobs to use in parallelization \n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian grid fit\n",
    "The first stage is the 'grid fit'\n",
    "Here we make a \"grid\" of possible PRF models, (different locations: polar angle, eccentricity, as well as sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eccentricity = prf_stim.screen_size_degrees/2 # It doesn't make sense to look for PRFs which are outside the stimulated region\n",
    "grid_nr = prf_info['grid_nr'] # Size of the grid (i.e., number of possible PRF models). Higher number means that the grid fit will be more exact, but take longer...\n",
    "eccs    = max_eccentricity * np.linspace(0.25, 1, grid_nr)**2 # Squared because of cortical magnification, more efficiently tiles the visual field...\n",
    "sizes   = max_eccentricity * np.linspace(0.1, 1, grid_nr)**2  # Possible size values (i.e., sigma in gaussian model) \n",
    "polars  = np.linspace(0, 2*np.pi, grid_nr)              # Possible polar angle coordinates\n",
    "\n",
    "# We can also fit the hrf in the same way (specifically the derivative)\n",
    "# -> make a grid between 0-10 (see settings file)\n",
    "hrf_1_grid = np.linspace(prf_info['hrf']['deriv_bound'][0], prf_info['hrf']['deriv_bound'][1], 5)\n",
    "# We generally recommend to fix the dispersion value to 0\n",
    "hrf_2_grid = np.array([0.0])\n",
    "\n",
    "# If you don't want to do any hrf fitting - set both to be NONE (uncomment below)\n",
    "# hrf_1_grid = None\n",
    "# hrf_2_grid = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to put some boundaries on possible values\n",
    "We set the baseline to 0, so we want to fix that here. 'fixed_grid_baseline=0'\n",
    "In addition, there is the amplitude parameter (which scales the response). \n",
    "We want to put an upper limit for this too, so that we don't get strange responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amplitude bounds for gauss grid fit - set [min, max]\n",
    "gauss_grid_bounds = [[prf_info['prf_ampl'][0],prf_info['prf_ampl'][1]]] \n",
    "\n",
    "\n",
    "gf.grid_fit(\n",
    "    ecc_grid=eccs,\n",
    "    polar_grid=polars,\n",
    "    size_grid=sizes,\n",
    "    hrf_1_grid=hrf_1_grid,\n",
    "    hrf_2_grid=hrf_2_grid,\n",
    "    verbose=True,\n",
    "    n_batches=prf_info['n_batches'],               # The grid fit is performed in parallel over n_batches of units.Batch parallelization is faster than single-unit parallelization and of sequential computing.\n",
    "    fixed_grid_baseline=prf_info['fixed_grid_baseline'], # Fix the baseline? This makes sense if we have fixed the baseline in preprocessing\n",
    "    grid_bounds=gauss_grid_bounds\n",
    "    )\n",
    "\n",
    "print(f'Mean rsq = {gf.gridsearch_r2.mean():.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Iterative Fit\n",
    "Now we can do the iterative fit. This takes the best fitting grid (from the above stage), and iteratively tweaks the parameters until the best fit is founds. \n",
    "This takes a bit longer than the grid fit. We also need to setup the bounds for all the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_bounds = [\n",
    "    (-1.5*max_eccentricity, 1.5*max_eccentricity),          # x bound\n",
    "    (-1.5*max_eccentricity, 1.5*max_eccentricity),          # y bound\n",
    "    (1e-1, max_eccentricity*3),                             # prf size bounds\n",
    "    (prf_info['prf_ampl'][0],prf_info['prf_ampl'][1]),      # prf amplitude\n",
    "    (prf_info['bold_bsl'][0],prf_info['bold_bsl'][1]),      # bold baseline (fixed)\n",
    "    (prf_info['hrf']['deriv_bound'][0], prf_info['hrf']['deriv_bound'][1]), # hrf_1 bound\n",
    "    (prf_info['hrf']['disp_bound'][0], prf_info['hrf']['disp_bound'][1]), # hrf_2 bound\n",
    "]\n",
    "\n",
    "# Constraints determines which scipy fitter is used\n",
    "# -> can also be used to make certain parameters interdependent (e.g. size depening on eccentricity... not normally done)\n",
    "if prf_info['constraints']:\n",
    "    g_constraints = []   # uses trust-constraint (slower, but moves further from grid\n",
    "else:\n",
    "    g_constraints = None # uses l-BFGS (which is faster)\n",
    "\n",
    "gf.iterative_fit(\n",
    "    rsq_threshold=prf_info['rsq_threshold'],    # Minimum variance explained. Puts a lower bound on the quality of PRF fits. Any fits worse than this are thrown away...     \n",
    "    verbose=True,\n",
    "    bounds=gauss_bounds,       # Bounds (on parameters)\n",
    "    constraints=g_constraints, # Constraints\n",
    "    xtol=float(prf_info['xtol']),     # float, passed to fitting routine numerical tolerance on x\n",
    "    ftol=float(prf_info['ftol']),     # float, passed to fitting routine numerical tolerance on function\n",
    "    )               \n",
    "# Xtol and Ftol determine when the fitter stops trying to improve the fit. Set to higher values to to terminate sooner and speed up the fitting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELL DONE!\n",
    "We now have a set of prf fits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters are stored as a np.ndarray with units x parameters\n",
    "# gf.iterative_search_params[:,0] -> x position of prf\n",
    "# gf.iterative_search_params[:,1] -> y position of prf\n",
    "# gf.iterative_search_params[:,2] -> size of prf\n",
    "# gf.iterative_search_params[:,3] -> amplitude of prf\n",
    "# gf.iterative_search_params[:,4] -> baseline of prf\n",
    "# gf.iterative_search_params[:,5] -> rsq of prf\n",
    "\n",
    "# Plot the location...\n",
    "plt.scatter(gf.iterative_search_params[:,0], gf.iterative_search_params[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create the predicted timeseries, and compare these with the data\n",
    "prf_params = gf.iterative_search_params\n",
    "pred_tc = gg.return_prediction(\n",
    "    mu_x = prf_params[:,0], # x position\n",
    "    mu_y = prf_params[:,1], # y position\n",
    "    size = prf_params[:,2], # prf size\n",
    "    beta = prf_params[:,3], # prf amplitude\n",
    "    baseline = prf_params[:,4], # prf baseline (set to 0)\n",
    "    hrf_1 = prf_params[:,5],\n",
    "    hrf_2 = prf_params[:,6],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the model fits:\n",
    "* vary the voxel you are looking at (i_vx)\n",
    "* also look at how the parameters (in the title), determine the location and size of the RF\n",
    "* varying \"time_pt\", you can see where the stimulus is, at different times\n",
    "* Note that the HRF means that responses of the model will be delayed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(20,5))\n",
    "subfigs = fig.subfigures(1, 2, width_ratios=[10,30])\n",
    "i_vx = 99\n",
    "\n",
    "# ************* TIME COURSE PLOT *************\n",
    "time_pts = np.arange(psc_avg_ts.shape[1]) * TR_s    \n",
    "ax2 = subfigs[1].add_subplot()\n",
    "ax2.plot(time_pts, psc_avg_ts[i_vx,:], '-+k', label= 'data')\n",
    "ax2.plot(time_pts, pred_tc[i_vx,:], '-*r', label= 'pred')\n",
    "ax2.set_xlabel('time (s)')\n",
    "ax2.legend()\n",
    "\n",
    "gauss_param_name = ['x', 'y', 'size', 'beta', 'baseline', 'rsq']\n",
    "title_txt = ''\n",
    "for i,p in enumerate(gauss_param_name):\n",
    "    title_txt += f'{p}={prf_params[i_vx,i]:.3f},'\n",
    "ax2.set_title(title_txt)\n",
    "\n",
    "# ********* PRF PLOT **********\n",
    "rf = gauss2D_iso_cart(\n",
    "    x=prf_stim.x_coordinates,\n",
    "    y=prf_stim.y_coordinates,\n",
    "    mu=(prf_params[i_vx,0], prf_params[i_vx,1]),\n",
    "    sigma=prf_params[i_vx,2])\n",
    "ax1 = subfigs[0].add_subplot()\n",
    "ax1.imshow(rf, vmin=0, vmax=1, extent=[-5,5,-5,5])\n",
    "ax1.axis('off')\n",
    "\n",
    "time_pt = 40\n",
    "if time_pt != None:\n",
    "    ax1.imshow(dm[:,:,time_pt], vmin=0, vmax=1,cmap='Greys', alpha=0.1, extent=[-5,5,-5,5])\n",
    "    ax_lim = ax2.get_ylim()\n",
    "    ax2.plot((time_pt*TR_s, time_pt*TR_s), (ax_lim), 'k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
